{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aff4a611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv file\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e903ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568454, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88811f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id',\n",
       " 'ProductId',\n",
       " 'UserId',\n",
       " 'ProfileName',\n",
       " 'HelpfulnessNumerator',\n",
       " 'HelpfulnessDenominator',\n",
       " 'Score',\n",
       " 'Time',\n",
       " 'Summary',\n",
       " 'Text']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2ce91e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values n duplicates\n",
    "\n",
    "df = df[[\"Text\", \"Summary\"]].dropna().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9978d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(394967, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "facfefc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add length columns\n",
    "df[\"text_len\"] = df[\"Text\"].apply(lambda x: len(str(x).split()))\n",
    "df[\"summary_len\"] = df[\"Summary\"].apply(lambda x: len(str(x).split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fe84715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter: keep only reasonably long reviews and summaries\n",
    "filtered_df = df[(df[\"text_len\"] > 100) & (df[\"summary_len\"] > 5)]\n",
    "\n",
    "# Sample 10,000 rows randomly\n",
    "sampled_df = filtered_df.sample(n=1000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Drop extra columns now\n",
    "final_df = sampled_df[[\"Text\", \"Summary\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4234e6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final dataset size: (1000, 2)\n",
      "                                                Text  \\\n",
      "0  My daughter is 7 months old, and we started ba...   \n",
      "1  I starter drinking Milo since I was two years ...   \n",
      "2  Ingredients: Spices, salt, onion, paprika, gar...   \n",
      "\n",
      "                                             Summary  \n",
      "0  portable, liked by a kid who doesn't like gree...  \n",
      "1  Liked It When I was a Kid; Enjoying it Now tha...  \n",
      "2  Here are the actual ingredients from the packa...  \n"
     ]
    }
   ],
   "source": [
    "# Check shape\n",
    "print(f\"✅ Final dataset size: {final_df.shape}\")\n",
    "print(final_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d04b525",
   "metadata": {},
   "source": [
    "### Prepare the data for T5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c3e576f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          input_text  \\\n",
      "0  summarize: My daughter is 7 months old, and we...   \n",
      "1  summarize: I starter drinking Milo since I was...   \n",
      "\n",
      "                                         target_text  \n",
      "0  portable, liked by a kid who doesn't like gree...  \n",
      "1  Liked It When I was a Kid; Enjoying it Now tha...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nethma\\AppData\\Local\\Temp\\ipykernel_4472\\3320227742.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df[\"input_text\"] = \"summarize: \" + final_df[\"Text\"]\n"
     ]
    }
   ],
   "source": [
    "# Add T5-format input and target columns\n",
    "final_df[\"input_text\"] = \"summarize: \" + final_df[\"Text\"]\n",
    "final_df[\"target_text\"] = final_df[\"Summary\"]\n",
    "\n",
    "# Drop old columns to keep only what's needed\n",
    "t5_df = final_df[[\"input_text\", \"target_text\"]]\n",
    "print(t5_df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adf2ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_df.to_csv(\"t5_amazon_10k.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df514ead",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7c3d93a",
   "metadata": {},
   "source": [
    "### Tokenize the dataset for T5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4f08294",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Tokenizer and Define Preprocessing Function\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer for T5-small\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Define preprocessing/tokenization function\n",
    "def preprocess_function(example):\n",
    "    model_inputs = tokenizer(\n",
    "        example[\"input_text\"],\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        example[\"target_text\"],\n",
    "        max_length=64,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "836214df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42e8c86efae435f843884d55f5d912b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_text': \"summarize: My daughter is 7 months old, and we started baby food right before 6 months. She has never been a fan of most green things (spring veggie mixes, green beans, etc) even though we keep trying them. But she likes this! It tastes more like pear sauce than anything (I tried it).<br /><br />It's also nicely portable. My little one is too young to squeeze it right into her mouth, but I can carefully squeeze some onto a spoon if we're out (if I'm home, I usually just squeeze it into a bowl). I also reseals well (the cap screws back on) and is easily tossed back into the diaper back if it's not finished. I would definitely buy this again and in other flavors.\",\n",
       " 'target_text': \"portable, liked by a kid who doesn't like green things\",\n",
       " 'input_ids': [21603,\n",
       "  10,\n",
       "  499,\n",
       "  3062,\n",
       "  19,\n",
       "  489,\n",
       "  767,\n",
       "  625,\n",
       "  6,\n",
       "  11,\n",
       "  62,\n",
       "  708,\n",
       "  1871,\n",
       "  542,\n",
       "  269,\n",
       "  274,\n",
       "  431,\n",
       "  767,\n",
       "  5,\n",
       "  451,\n",
       "  65,\n",
       "  470,\n",
       "  118,\n",
       "  3,\n",
       "  9,\n",
       "  1819,\n",
       "  13,\n",
       "  167,\n",
       "  1442,\n",
       "  378,\n",
       "  41,\n",
       "  14662,\n",
       "  30642,\n",
       "  25190,\n",
       "  6,\n",
       "  1442,\n",
       "  8845,\n",
       "  6,\n",
       "  672,\n",
       "  61,\n",
       "  237,\n",
       "  713,\n",
       "  62,\n",
       "  453,\n",
       "  1119,\n",
       "  135,\n",
       "  5,\n",
       "  299,\n",
       "  255,\n",
       "  114,\n",
       "  7,\n",
       "  48,\n",
       "  55,\n",
       "  94,\n",
       "  13434,\n",
       "  72,\n",
       "  114,\n",
       "  158,\n",
       "  291,\n",
       "  3837,\n",
       "  145,\n",
       "  959,\n",
       "  41,\n",
       "  196,\n",
       "  1971,\n",
       "  34,\n",
       "  137,\n",
       "  2,\n",
       "  115,\n",
       "  52,\n",
       "  3,\n",
       "  87,\n",
       "  3155,\n",
       "  2,\n",
       "  115,\n",
       "  52,\n",
       "  3,\n",
       "  87,\n",
       "  3155,\n",
       "  196,\n",
       "  17,\n",
       "  31,\n",
       "  7,\n",
       "  92,\n",
       "  11040,\n",
       "  6262,\n",
       "  5,\n",
       "  499,\n",
       "  385,\n",
       "  80,\n",
       "  19,\n",
       "  396,\n",
       "  1021,\n",
       "  12,\n",
       "  20103,\n",
       "  34,\n",
       "  269,\n",
       "  139,\n",
       "  160,\n",
       "  4247,\n",
       "  6,\n",
       "  68,\n",
       "  27,\n",
       "  54,\n",
       "  4321,\n",
       "  20103,\n",
       "  128,\n",
       "  2400,\n",
       "  3,\n",
       "  9,\n",
       "  14987,\n",
       "  3,\n",
       "  99,\n",
       "  62,\n",
       "  31,\n",
       "  60,\n",
       "  91,\n",
       "  41,\n",
       "  99,\n",
       "  27,\n",
       "  31,\n",
       "  51,\n",
       "  234,\n",
       "  6,\n",
       "  27,\n",
       "  1086,\n",
       "  131,\n",
       "  20103,\n",
       "  34,\n",
       "  139,\n",
       "  3,\n",
       "  9,\n",
       "  3047,\n",
       "  137,\n",
       "  27,\n",
       "  92,\n",
       "  3,\n",
       "  60,\n",
       "  7,\n",
       "  15,\n",
       "  5405,\n",
       "  168,\n",
       "  41,\n",
       "  532,\n",
       "  2468,\n",
       "  19572,\n",
       "  223,\n",
       "  30,\n",
       "  61,\n",
       "  11,\n",
       "  19,\n",
       "  1153,\n",
       "  3,\n",
       "  27057,\n",
       "  223,\n",
       "  139,\n",
       "  8,\n",
       "  29375,\n",
       "  223,\n",
       "  3,\n",
       "  99,\n",
       "  34,\n",
       "  31,\n",
       "  7,\n",
       "  59,\n",
       "  2369,\n",
       "  5,\n",
       "  27,\n",
       "  133,\n",
       "  1728,\n",
       "  805,\n",
       "  48,\n",
       "  541,\n",
       "  11,\n",
       "  16,\n",
       "  119,\n",
       "  11104,\n",
       "  5,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [6262,\n",
       "  6,\n",
       "  6528,\n",
       "  57,\n",
       "  3,\n",
       "  9,\n",
       "  4984,\n",
       "  113,\n",
       "  744,\n",
       "  31,\n",
       "  17,\n",
       "  114,\n",
       "  1442,\n",
       "  378,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Convert pandas DataFrame to Hugging Face Dataset\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert to Hugging Face Dataset object\n",
    "dataset = Dataset.from_pandas(t5_df)\n",
    "\n",
    "# Tokenize\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Check sample\n",
    "tokenized_dataset[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b7939a",
   "metadata": {},
   "source": [
    "### Fine-Tune t5-small Using Hugging Face Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0210c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the model\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "902e0aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define ROUGE for summarization\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Rouge expects newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(pred.strip().split(\". \")) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(label.strip().split(\". \")) for label in decoded_labels]\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return {k: round(v * 100, 2) for k, v in result.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b451151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split dataset\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65b70282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Nethma\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nethma\\AppData\\Local\\Temp\\ipykernel_4472\\1538940811.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "## TrainingArguments + Trainer\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5_summarizer_results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    # save_strategy=\"epoch\",\n",
    "    # logging_dir=\"./logs\",\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    # predict_with_generate=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6d671cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 09:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=3.4182504272460936, metrics={'train_runtime': 551.7132, 'train_samples_per_second': 1.45, 'train_steps_per_second': 0.181, 'total_flos': 108273441177600.0, 'train_loss': 3.4182504272460936, 'epoch': 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TRain the model\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35dc7cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./t5_summarizer_model\\\\tokenizer_config.json',\n",
       " './t5_summarizer_model\\\\special_tokens_map.json',\n",
       " './t5_summarizer_model\\\\spiece.model',\n",
       " './t5_summarizer_model\\\\added_tokens.json',\n",
       " './t5_summarizer_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "trainer.save_model(\"./t5_summarizer_model\")\n",
    "tokenizer.save_pretrained(\"./t5_summarizer_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deea4d1c",
   "metadata": {},
   "source": [
    "## Run Predictions with Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d36a75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load some test examples\n",
    "\n",
    "# Take some test samples from your original dataset (not tokenized)\n",
    "import random\n",
    "\n",
    "sample_texts = final_df.sample(5, random_state=42)[\"Text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1307df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "## Define a prediction function\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"./t5_summarizer_model\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"./t5_summarizer_model\")\n",
    "model.eval()\n",
    "\n",
    "def summarize(text, max_input_length=512, max_target_length=64):\n",
    "    input_text = \"summarize: \" + text\n",
    "    input_ids = tokenizer.encode(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=max_input_length,\n",
    "        truncation=True\n",
    "    ).to(model.device)\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_target_length,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad617f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Review 1:\n",
      " Original:\n",
      "Ok, my kid is VERY finicky. Things have to be made a certain way, etc. She also has food allergies, which in turn exacerbate her eczema, as well as seriously limiting what she can eat, even healthy stuff. Flax is good for eczema, but she doesn't like it added to cereal, won't do the oil, even when mixed w/maple syrup, etc. Enter these WONDERFUL crackers. Dehydrated at low temps to preserve the oil (& thus all the nutritional benefits), with no artificial colors or flavors, no gluten or other com...\n",
      "\n",
      " Summary:\n",
      "\n",
      "\n",
      "🔹 Review 2:\n",
      " Original:\n",
      "I've tried this tea by the bag and it does tend to be milder than your average supermarket Tetley or Lipton. But I've been drinking Bigelow Green Tea that way for about a year and I wanted to revisit Earl Grey via a Keurig-clone machine I got for my birthday over the summer. After I went through a couple of \"variety packs\" I picked up an Ekobrew Refillable basket and started drinking expresso done that way for my morning coffee. I admit, using a Keurig machine is still a bit low tech compared to...\n",
      "\n",
      " Summary:\n",
      "\n",
      "\n",
      "🔹 Review 3:\n",
      " Original:\n",
      "I am a Rooibos junkie. I have been drinking it for a few years and adore it. I can only tolerate limited caffeine, early in my day or I'll be up and cranky and not ready to face the world the following day.  Rooibos, in a regular tea strength, with milk is a caffeine free and still tasty (tastes like black tea, when brewed properly, with a little less of the note of bitterness)...<br /><br />I also love espresso drinks and worked for a long time as a barista in a cafe. So I figured this would be...\n",
      "\n",
      " Summary:\n",
      "\n",
      "\n",
      "🔹 Review 4:\n",
      " Original:\n",
      "Mum mums are easy to feed our infant- he can do it himself! He stuffs these things in!<br /><br />They can get a little messy (like all infant food) and stick into things when wet.  If you clean them up while wet- no problem- but if they dry into clothes or the carpet they are pain.<br /><br />They taste pretty good (better than infant cereal!)and even my toddler asks for them- and in a pinch I've snacked on a few too.<br /><br />The only problem-  the packaging.  I ordered them by the 6 box cas...\n",
      "\n",
      " Summary:\n",
      "\n",
      "\n",
      "🔹 Review 5:\n",
      " Original:\n",
      "Okay, the doctor said, after the second heart attack that some changes could help. Caffeine was one area. But I did not want to give up my coffee, and it was always Starbucks (home brew), no store in our small town. I've been hooked on the strength and goodness of Sumatra for years. No restaurant in our area makes coffee as good as my wife makes my home brew Sumatra. So I give Decaf Sumatra from Starbucks a try.<br /><br />HONEST, NO LIE, I COULD NOT TELL THE DIFFERENCE. I even tried mixing half...\n",
      "\n",
      " Summary:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Run predictions\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    print(f\"\\n🔹 Review {i+1}:\")\n",
    "    print(f\" Original:\\n{text[:500]}...\\n\")\n",
    "    print(f\" Summary:\\n{summarize(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a7abf7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
